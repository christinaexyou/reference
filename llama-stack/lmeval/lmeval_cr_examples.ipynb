{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LMEval Custom Resource Generation Examples\n",
        "\n",
        "This notebook demonstrates how to generate LMEval Custom Resources (CRs) without deploying them to Kubernetes.\n",
        "\n",
        "Start by installing the necessary dependencies:\n",
        "\n",
        "```bash\n",
        "pip install llama-stack-provider-lmeval\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "sys.path.insert(0, os.path.join(os.path.dirname('.'), 'src'))\n",
        "\n",
        "from llama_stack_provider_lmeval.lmeval import (\n",
        "    LMEvalCRBuilder, LMEvalCR, LMEvalSpec, LMEvalMetadata, TaskList, ModelArg\n",
        ")\n",
        "\n",
        "def dict_to_yaml(d: Dict[Any, Any], indent: int = 0) -> str:\n",
        "    \"\"\"Convert dictionary to YAML-like format.\"\"\"\n",
        "    yaml_str = \"\"\n",
        "    for key, value in d.items():\n",
        "        if isinstance(value, dict):\n",
        "            yaml_str += \"  \" * indent + f\"{key}:\\n\"\n",
        "            yaml_str += dict_to_yaml(value, indent + 1)\n",
        "        elif isinstance(value, list):\n",
        "            yaml_str += \"  \" * indent + f\"{key}:\\n\"\n",
        "            for item in value:\n",
        "                if isinstance(item, dict):\n",
        "                    yaml_str += \"  \" * (indent + 1) + \"-\\n\"\n",
        "                    yaml_str += dict_to_yaml(item, indent + 2)\n",
        "                else:\n",
        "                    yaml_str += \"  \" * (indent + 1) + f\"- {item}\\n\"\n",
        "        elif isinstance(value, bool):\n",
        "            yaml_str += \"  \" * indent + f\"{key}: {str(value).lower()}\\n\"\n",
        "        elif value is None:\n",
        "            yaml_str += \"  \" * indent + f\"{key}: null\\n\"\n",
        "        else:\n",
        "            yaml_str += \"  \" * indent + f\"{key}: {value}\\n\"\n",
        "    return yaml_str\n",
        "\n",
        "def display_cr_yaml(cr_dict: Dict[Any, Any]) -> None:\n",
        "    \"\"\"Display CR as YAML with nice formatting.\"\"\"\n",
        "    yaml_output = dict_to_yaml(cr_dict)\n",
        "    print(\"```yaml\")\n",
        "    print(yaml_output.rstrip())\n",
        "    print(\"```\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LMEvalCRBuilder initialised for namespace: lmeval-demo\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Using the existing LMEvalCRBuilder\n",
        "# This uses your production infrastructure to generate CRs\n",
        "\n",
        "def create_mock_benchmark_config():\n",
        "    \"\"\"Create a mock benchmark config for testing purposes.\"\"\"\n",
        "    class MockEvalCandidate:\n",
        "        def __init__(self):\n",
        "            self.type = \"model\"\n",
        "            self.model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "            self.sampling_params = {}\n",
        "\n",
        "    class MockBenchmarkConfig:\n",
        "        def __init__(self):\n",
        "            self.eval_candidate = MockEvalCandidate()\n",
        "            self.dataset = {\"identifier\": \"dk_bench\"}\n",
        "            self.scoring_params = {}\n",
        "            self.env_vars = []\n",
        "            self.metadata = {}\n",
        "    return MockBenchmarkConfig()\n",
        "\n",
        "# Create the CR builder\n",
        "namespace = \"lmeval-demo\"\n",
        "cr_builder = LMEvalCRBuilder(namespace=namespace)\n",
        "mock_config = create_mock_benchmark_config()\n",
        "\n",
        "print(f\"LMEvalCRBuilder initialised for namespace: {namespace}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 1: Basic LMEval CR\n",
        "\n",
        "This generates a standard evaluation CR for the Phi-3 model with dk_bench tasks.\n",
        "\n",
        "### Expected YAML Output:\n",
        "```yaml\n",
        "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
        "kind: LMEvalJob\n",
        "metadata:\n",
        "  name: basic-evaluation-001\n",
        "  namespace: lmeval-demo\n",
        "spec:\n",
        "  allowOnline: true\n",
        "  allowCodeExecution: true\n",
        "  model: local-completions\n",
        "  taskList:\n",
        "    taskNames:\n",
        "    - dk_bench\n",
        "  logSamples: true\n",
        "  batchSize: \"1\"\n",
        "  modelArgs:\n",
        "  - name: model\n",
        "    value: microsoft/Phi-3-mini-4k-instruct\n",
        "  - name: num_concurrent\n",
        "    value: \"1\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Generating Basic LMEval CR...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">09:59:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">399</span> llama_stack_provider_lmeval.lmeval:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">508</span> uncategorized: No git source data found for     \n",
              "         customTasks                                                                                                    \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[33mWARNING \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m17\u001b[0m \u001b[1;92m09:59:36\u001b[0m,\u001b[1;36m399\u001b[0m llama_stack_provider_lmeval.lmeval:\u001b[1;36m508\u001b[0m uncategorized: No git source data found for     \n",
              "         customTasks                                                                                                    \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‹ Basic LMEval CR (YAML format):\n",
            "==================================================\n",
            "```yaml\n",
            "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
            "kind: LMEvalJob\n",
            "metadata:\n",
            "  name: lmeval-llama-stack-job-8d14ea58\n",
            "  namespace: lmeval-demo\n",
            "spec:\n",
            "  allowOnline: true\n",
            "  allowCodeExecution: true\n",
            "  model: local-completions\n",
            "  taskList:\n",
            "    taskNames:\n",
            "      - basic-evaluation-001\n",
            "  logSamples: true\n",
            "  batchSize: 1\n",
            "  limit: null\n",
            "  modelArgs:\n",
            "    -\n",
            "      name: model\n",
            "      value: microsoft/Phi-3-mini-4k-instruct\n",
            "    -\n",
            "      name: num_concurrent\n",
            "      value: 1\n",
            "  pod: null\n",
            "  offline: null\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ”„ Generating Basic LMEval CR...\")\n",
        "\n",
        "# Generate basic CR\n",
        "basic_cr = cr_builder.create_cr(\n",
        "    benchmark_id=\"basic-evaluation-001\",\n",
        "    task_config=mock_config,\n",
        "    base_url=None,\n",
        "    limit=None\n",
        ")\n",
        "\n",
        "print(\"ðŸ“‹ Basic LMEval CR (YAML format):\")\n",
        "print(\"=\" * 50)\n",
        "display_cr_yaml(basic_cr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 2: External Model Service CR\n",
        "\n",
        "This generates a CR that points to an external model service (like your deployed Phi-3 predictor).\n",
        "\n",
        "### Expected YAML Output:\n",
        "```yaml\n",
        "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
        "kind: LMEvalJob\n",
        "metadata:\n",
        "  name: external-model-evaluation-001\n",
        "  namespace: lmeval-demo\n",
        "spec:\n",
        "  allowOnline: true\n",
        "  allowCodeExecution: true\n",
        "  model: local-completions\n",
        "  taskList:\n",
        "    taskNames:\n",
        "    - dk_bench\n",
        "  logSamples: true\n",
        "  batchSize: \"1\"\n",
        "  modelArgs:\n",
        "  - name: model\n",
        "    value: microsoft/Phi-3-mini-4k-instruct\n",
        "  - name: base_url\n",
        "    value: https://phi-3-predictor-example.apps.cluster.local/v1/openai/v1/completions\n",
        "  - name: num_concurrent\n",
        "    value: \"1\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating External Model Service CR...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">09:59:53</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">349</span> llama_stack_provider_lmeval.lmeval:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">508</span> uncategorized: No git source data found for     \n",
              "         customTasks                                                                                                    \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[33mWARNING \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m17\u001b[0m \u001b[1;92m09:59:53\u001b[0m,\u001b[1;36m349\u001b[0m llama_stack_provider_lmeval.lmeval:\u001b[1;36m508\u001b[0m uncategorized: No git source data found for     \n",
              "         customTasks                                                                                                    \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "External Model Service CR (YAML format):\n",
            "==================================================\n",
            "```yaml\n",
            "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
            "kind: LMEvalJob\n",
            "metadata:\n",
            "  name: lmeval-llama-stack-job-32373d38\n",
            "  namespace: lmeval-demo\n",
            "spec:\n",
            "  allowOnline: true\n",
            "  allowCodeExecution: true\n",
            "  model: local-completions\n",
            "  taskList:\n",
            "    taskNames:\n",
            "      - external-model-evaluation-001\n",
            "  logSamples: true\n",
            "  batchSize: 1\n",
            "  limit: null\n",
            "  modelArgs:\n",
            "    -\n",
            "      name: model\n",
            "      value: microsoft/Phi-3-mini-4k-instruct\n",
            "    -\n",
            "      name: base_url\n",
            "      value: https://phi-3-predictor-example.apps.cluster.local/v1/openai/v1/completions\n",
            "    -\n",
            "      name: num_concurrent\n",
            "      value: 1\n",
            "  pod: null\n",
            "  offline: null\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating External Model Service CR...\")\n",
        "\n",
        "# Generate CR with external model service\n",
        "external_cr = cr_builder.create_cr(\n",
        "    benchmark_id=\"external-model-evaluation-001\",\n",
        "    task_config=mock_config,\n",
        "    base_url=\"https://phi-3-predictor-example.apps.cluster.local\",\n",
        "    limit=None\n",
        ")\n",
        "\n",
        "print(\"External Model Service CR (YAML format):\")\n",
        "print(\"=\" * 50)\n",
        "display_cr_yaml(external_cr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 3: Limited Sample Evaluation CR\n",
        "\n",
        "This generates a CR with a limit on the number of samples to evaluate.\n",
        "\n",
        "### Expected YAML Output:\n",
        "```yaml\n",
        "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
        "kind: LMEvalJob\n",
        "metadata:\n",
        "  name: limited-evaluation-001\n",
        "  namespace: lmeval-demo\n",
        "spec:\n",
        "  allowOnline: true\n",
        "  allowCodeExecution: true\n",
        "  model: local-completions\n",
        "  taskList:\n",
        "    taskNames:\n",
        "    - dk_bench\n",
        "  logSamples: true\n",
        "  batchSize: \"1\"\n",
        "  limit: \"50\"\n",
        "  modelArgs:\n",
        "  - name: model\n",
        "    value: microsoft/Phi-3-mini-4k-instruct\n",
        "  - name: num_concurrent\n",
        "    value: \"1\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating Limited Sample Evaluation CR...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">09:59:59</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">831</span> llama_stack_provider_lmeval.lmeval:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">508</span> uncategorized: No git source data found for     \n",
              "         customTasks                                                                                                    \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[33mWARNING \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m17\u001b[0m \u001b[1;92m09:59:59\u001b[0m,\u001b[1;36m831\u001b[0m llama_stack_provider_lmeval.lmeval:\u001b[1;36m508\u001b[0m uncategorized: No git source data found for     \n",
              "         customTasks                                                                                                    \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Limited Sample Evaluation CR (YAML format):\n",
            "==================================================\n",
            "```yaml\n",
            "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
            "kind: LMEvalJob\n",
            "metadata:\n",
            "  name: lmeval-llama-stack-job-2260943c\n",
            "  namespace: lmeval-demo\n",
            "spec:\n",
            "  allowOnline: true\n",
            "  allowCodeExecution: true\n",
            "  model: local-completions\n",
            "  taskList:\n",
            "    taskNames:\n",
            "      - limited-evaluation-001\n",
            "  logSamples: true\n",
            "  batchSize: 1\n",
            "  limit: 50\n",
            "  modelArgs:\n",
            "    -\n",
            "      name: model\n",
            "      value: microsoft/Phi-3-mini-4k-instruct\n",
            "    -\n",
            "      name: num_concurrent\n",
            "      value: 1\n",
            "  pod: null\n",
            "  offline: null\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating Limited Sample Evaluation CR...\")\n",
        "\n",
        "# Generate CR with sample limit\n",
        "limited_cr = cr_builder.create_cr(\n",
        "    benchmark_id=\"limited-evaluation-001\",\n",
        "    task_config=mock_config,\n",
        "    base_url=None,\n",
        "    limit=\"50\"  # Only evaluate 50 samples\n",
        ")\n",
        "\n",
        "print(\"Limited Sample Evaluation CR (YAML format):\")\n",
        "print(\"=\" * 50)\n",
        "display_cr_yaml(limited_cr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Method 2: Using Pydantic Models Directly\n",
        "\n",
        "This shows how to build CRs using the Pydantic models directly, including configurations like offline storage and custom tasks.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function for direct CR construction using Pydantic models\n",
        "def create_basic_cr_dict(\n",
        "    name: str = \"example-lmeval\",\n",
        "    namespace: str = \"default\",\n",
        "    model_name: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    task_names: List[str] = None,\n",
        "    base_url: Optional[str] = None,\n",
        "    limit: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Create a basic LMEval CR dictionary structure.\"\"\"\n",
        "    if task_names is None:\n",
        "        task_names = [\"dk_bench\"]\n",
        "\n",
        "    # Create model args\n",
        "    model_args = [ModelArg(name=\"model\", value=model_name)]\n",
        "    if base_url:\n",
        "        base_url = base_url.rstrip(\"/\")\n",
        "        openai_base_url = f\"{base_url}/v1\"\n",
        "        model_args.append(ModelArg(name=\"base_url\", value=openai_base_url))\n",
        "    model_args.append(ModelArg(name=\"num_concurrent\", value=\"1\"))\n",
        "\n",
        "    # Create task list\n",
        "    task_list = TaskList(taskNames=task_names)\n",
        "\n",
        "    # Create spec\n",
        "    spec = LMEvalSpec(\n",
        "        model=\"local-completions\",\n",
        "        taskList=task_list,\n",
        "        logSamples=True,\n",
        "        batchSize=\"1\",\n",
        "        limit=limit,\n",
        "        modelArgs=model_args\n",
        "    )\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = LMEvalMetadata(name=name, namespace=namespace)\n",
        "\n",
        "    # Create the full CR\n",
        "    cr = LMEvalCR(metadata=metadata, spec=spec)\n",
        "\n",
        "    return cr.model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 4: CR with Offline Storage\n",
        "\n",
        "This creates a CR configured to use persistent storage for offline evaluation data.\n",
        "\n",
        "### Expected YAML Output:\n",
        "```yaml\n",
        "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
        "kind: LMEvalJob\n",
        "metadata:\n",
        "  name: offline-evaluation\n",
        "  namespace: lmeval-demo\n",
        "spec:\n",
        "  allowOnline: true\n",
        "  allowCodeExecution: true\n",
        "  model: local-completions\n",
        "  taskList:\n",
        "    taskNames:\n",
        "    - dk_bench\n",
        "  logSamples: true\n",
        "  batchSize: \"1\"\n",
        "  modelArgs:\n",
        "  - name: model\n",
        "    value: microsoft/Phi-3-mini-4k-instruct\n",
        "  - name: num_concurrent\n",
        "    value: \"1\"\n",
        "  offline:\n",
        "    storage:\n",
        "      pvcName: lmeval-storage-pvc\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating CR with Offline Storage...\n",
            "CR with Offline Storage (YAML format):\n",
            "==================================================\n",
            "```yaml\n",
            "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
            "kind: LMEvalJob\n",
            "metadata:\n",
            "  name: offline-evaluation\n",
            "  namespace: lmeval-demo\n",
            "spec:\n",
            "  allowOnline: true\n",
            "  allowCodeExecution: true\n",
            "  model: local-completions\n",
            "  taskList:\n",
            "    taskNames:\n",
            "      - dk_bench\n",
            "  logSamples: true\n",
            "  batchSize: 1\n",
            "  limit: null\n",
            "  modelArgs:\n",
            "    -\n",
            "      name: model\n",
            "      value: microsoft/Phi-3-mini-4k-instruct\n",
            "    -\n",
            "      name: num_concurrent\n",
            "      value: 1\n",
            "  pod: null\n",
            "  offline:\n",
            "    storage:\n",
            "      pvcName: lmeval-storage-pvc\n",
            "```\n",
            "\n",
            "Note: The offline.storage.pvcName references a PersistentVolumeClaim\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating CR with Offline Storage...\")\n",
        "\n",
        "# Create CR with offline storage\n",
        "offline_cr = create_basic_cr_dict(\n",
        "    name=\"offline-evaluation\",\n",
        "    namespace=\"lmeval-demo\",\n",
        "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    task_names=[\"dk_bench\"]\n",
        ")\n",
        "\n",
        "# Add offline storage configuration\n",
        "offline_cr[\"spec\"][\"offline\"] = {\"storage\": {\"pvcName\": \"lmeval-storage-pvc\"}}\n",
        "\n",
        "print(\"CR with Offline Storage (YAML format):\")\n",
        "print(\"=\" * 50)\n",
        "display_cr_yaml(offline_cr)\n",
        "\n",
        "print(\"\\nNote: The offline.storage.pvcName references a PersistentVolumeClaim\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 5: CR with Custom Tasks from Git\n",
        "\n",
        "This creates a CR that loads custom evaluation tasks from a Git repository.\n",
        "\n",
        "### Expected YAML Output:\n",
        "```yaml\n",
        "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
        "kind: LMEvalJob\n",
        "metadata:\n",
        "  name: custom-tasks-evaluation\n",
        "  namespace: lmeval-demo\n",
        "spec:\n",
        "  allowOnline: true\n",
        "  allowCodeExecution: true\n",
        "  model: local-completions\n",
        "  taskList:\n",
        "    taskNames:\n",
        "    - custom_task_1\n",
        "    - custom_task_2\n",
        "    customTasks:\n",
        "      source:\n",
        "        git:\n",
        "          url: https://github.com/example/custom-tasks.git\n",
        "          revision: main\n",
        "          directory: custom_tasks\n",
        "  logSamples: true\n",
        "  batchSize: \"1\"\n",
        "  modelArgs:\n",
        "  - name: model\n",
        "    value: microsoft/Phi-3-mini-4k-instruct\n",
        "  - name: num_concurrent\n",
        "    value: \"1\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating CR with Custom Tasks from Git...\n",
            "CR with Custom Tasks from Git (YAML format):\n",
            "==================================================\n",
            "```yaml\n",
            "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
            "kind: LMEvalJob\n",
            "metadata:\n",
            "  name: custom-tasks-evaluation\n",
            "  namespace: lmeval-demo\n",
            "spec:\n",
            "  allowOnline: true\n",
            "  allowCodeExecution: true\n",
            "  model: local-completions\n",
            "  taskList:\n",
            "    taskNames:\n",
            "      - custom_task_1\n",
            "      - custom_task_2\n",
            "    customTasks:\n",
            "      source:\n",
            "        git:\n",
            "          url: https://github.com/example/custom-tasks.git\n",
            "          revision: main\n",
            "          directory: custom_tasks\n",
            "  logSamples: true\n",
            "  batchSize: 1\n",
            "  limit: null\n",
            "  modelArgs:\n",
            "    -\n",
            "      name: model\n",
            "      value: microsoft/Phi-3-mini-4k-instruct\n",
            "    -\n",
            "      name: num_concurrent\n",
            "      value: 1\n",
            "  pod: null\n",
            "  offline: null\n",
            "```\n",
            "\n",
            "Note: Custom tasks are loaded from the specified Git repository\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating CR with Custom Tasks from Git...\")\n",
        "\n",
        "# Create CR with custom tasks\n",
        "custom_cr = create_basic_cr_dict(\n",
        "    name=\"custom-tasks-evaluation\",\n",
        "    namespace=\"lmeval-demo\",\n",
        "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    task_names=[\"custom_task_1\", \"custom_task_2\"]\n",
        ")\n",
        "\n",
        "# Add custom tasks configuration\n",
        "custom_cr[\"spec\"][\"taskList\"][\"customTasks\"] = {\n",
        "    \"source\": {\n",
        "        \"git\": {\n",
        "            \"url\": \"https://github.com/example/custom-tasks.git\",\n",
        "            \"revision\": \"main\",\n",
        "            \"directory\": \"custom_tasks\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"CR with Custom Tasks from Git (YAML format):\")\n",
        "print(\"=\" * 50)\n",
        "display_cr_yaml(custom_cr)\n",
        "\n",
        "print(\"\\nNote: Custom tasks are loaded from the specified Git repository\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
